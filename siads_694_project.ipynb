{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unsupervised Learning: Dimensionality Reduction, FAMD</h3>\n",
    "The purpose of this notebook is to read in the CDC survey data and try different dimensionality reduction methods on it to see if we can reduce the complexity of the dataset. The dataset has ~300 columns, we reduce that to 30 columns that we manually choose. Although 30 columns is less complex than 300 complex we want to reduce the complexity even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations needed, please uncomment and run if needed\n",
    "'''! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install zipfile\n",
    "! pip install plotly.express\n",
    "! pip install plotly.graph_objects\n",
    "! pip install plotly==5.5\n",
    "! pip install matplotlib.pyplot\n",
    "! pip install types\n",
    "! pip install prince'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile \n",
    "from prince import FAMD, MCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random_state for reproducible results\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in DataFrame\n",
    "zf = zipfile.ZipFile('ny.csv.zip') \n",
    "zf.namelist() \n",
    "df = pd.read_csv(zf.open('ny.csv'),  encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is pretty clean, however there are some columns where majority of the values are missing and we want to drop those columns to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_clean for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Repalce 'Not asked or Missing' and 'Data do not meet the criteria for statistical reliability, \n",
    "# data quality or confidentiality (data are suppressed)' with NA\n",
    "for col in df.columns:\n",
    "    df_clean[col].replace({'Not asked or Missing' : np.nan}, inplace = True)\n",
    "    df_clean[col].replace({'Data do not meet the criteria for statistical reliability, data quality or confidentiality (data are suppressed)' : np.nan}, inplace = True)\n",
    "    \n",
    "# Drop columns with over 80% missing values\n",
    "df_clean.dropna(axis = 1, thresh = len(df_clean) * .50, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to try manually selecting columns from the dataset based on literature that discusses which lifestyle questions are generally attributed to an increased risk of heart attacks. The sources used are listed in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns related to heart disease by utilizing resources detailing factors of heart disease\n",
    "# shorturl.at/oqwF5 - Behavioral risk factors of coronary artery disease: A paired matched case control study\n",
    "# shorturl.at/cpAXZ - Strategies to prevent heart disease\n",
    "# shorturl.at/gpwAR - Top five habits that harm the heart\n",
    "# shorturl.at/mtJUZ - 9 Common Habits That Are Bad for Your Heart\n",
    "\n",
    "# Characteristics\n",
    "# 1. SEXVAR - Sex - (Male or Female)\n",
    "# 2. _IMPAGE - Age - (Age 65 or older, Age 55 - 64, Age 45 - 54, Age 35 - 44, Age 25 - 34, Age 18 - 24)\n",
    "# 3. _IMPRACE - Race - (White, Non-Hispanic, Hispanic, Black, Non-Hispanic, Other race, Non-Hispanic, Asian, Non-Hispanic,\n",
    "# American Indian/Alaskan Native, Non-Hispanic)\n",
    "# 4. VETERAN3 - Former veteran status - (Yes, No, Refused, Don't know/Not sure)\n",
    "# 5. WTKG3 - Weight in KG - (Continous value)\n",
    "# 6. _IMPMRTL - Marital status - (Married, Never Married, Divorced, Widowed, A member of an unmarried couple, \n",
    "# Separated)\n",
    "# 7. _RFBMI5 - Overweight or Obese - (Yes, No, Don’t know/Refused/Missing)\n",
    "\n",
    "\n",
    "# Health \n",
    "# 8. HLTHPLN1 - Has Healthcare Coverage - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 9. ADDEPEV3 - Diagnosed with depression - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 10. DIABETE4 - Diagnosed with diabetes - (Yes, Yes, but female told only during pregnancy, \n",
    "# No, pre-diabetes or borderline diabetes, No, Don't know/Not sure, Refused)\n",
    "# 11. RMVTETH4 - Number of teeth removed - All, 6 or more, but not all, 1 to 5, None, Don't know/Not sure, Refused\n",
    "# 12. _PHYS14D - Number of days physical health not well - (Zero days when physical health not good,     \n",
    "# 1-13 days when physical health not good, 14+ days when physical health not good, Don’t know/Refused/Missing)                 \n",
    "# 13. _MENT14D - Number of days mental health not well - Zero days when mental health not good\n",
    "# 1-13 days when mental health not good, 14+ days when mental health not good, Don’t know/Refused/Missing    \n",
    "# 14. _TOTINDA - Physical activity - (Had physical activity or exercise, No physical activity or exercise in last 30 days,     \n",
    "# Don’t know/Refused/Missing)       \n",
    "# 15. PDIABTST - User has gotten a test for high blood sugar in past 3 years - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 16. PREDIAB1 - Diagnosed as prediabetic - Yes, Yes, during pregnancy, Don't know/Not Sure, Refused, No\n",
    "# 17. _RFHLTH - General health - (Good or Better Health, Fair or Poor Health, Don’t know/Not Sure Or Refused/Missing)\n",
    "# 18. BPHIGH4 - (Told they have high blood pressure - Yes, Told borderline high or pre-hypertensive, \n",
    "# Yes, but female told only during pregnancy, Don't Know/Not Sure Refused, No) \n",
    "\n",
    "# Lifestyle\n",
    "# 19. CHECKUP1 - Length since last checkup - (Within past year (anytime less than 12 months ago), \n",
    "# Within past 2 years (1 year but less than 2 years ago), Within past 5 years (2 years but less than 5 years ago), \n",
    "# 5 or more years ago, Don’t know/Not sure, Never, Refused)\n",
    "# 20. LASTDEN4 - Last visited dentist - (Within past year (anytime less than 12 months ago), \n",
    "# Within past 2 years (1 year but less than 2 years ago), Within past 5 years (2 years but less than 5 years ago), \n",
    "# 5 or more years ago, Don’t know/Not sure, Never, Refused)\n",
    "# 21. FLUSHOT7 - Whether someone has taken the flu shot - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 22. _RFSEAT3 - Seatbeat wearing status - (Always Wear Seat Belt, Don’t Always Wear Seat Belt\n",
    "# Don’t know/Not Sure Or Refused/Missing)\n",
    "\n",
    "# Socioeconomic status\n",
    "# 23. _IMPEDUC - Education - (College 4 years or more (College graduate), \n",
    "# College 1 year to 3 years (Some college or technical school), Grade 12 or GED (High school graduate), \n",
    "# Grades 9 through 11 (Some high school), Grades 1 through 8 (Elementary), Never attended school or only kindergarten)\n",
    "# Grades 9 through 11 (Some high school), Grades 1 through 8 (Elementary), Never attended school or only kindergarten)\n",
    "# 24. EMPLOY1 - \n",
    "# 25. _INCOMG - Income level - ($50,000 or more, Don’t know/Not sure/Missing, $15,000 to less than $25,000,   \n",
    "# $35,000 to less than $50,000, $25,000 to less than $35,000, Less than $15,000)\n",
    "# 26. _METSTAT - Whether they live in a metropolitan - (1, 2)\n",
    "\n",
    "# Tobacco, Alcohol\n",
    "# 27. USENOW3 - Use of smokeless tobacco - (Not at all, Some days, Every day, Refused, Don’t know/Not Sure) \n",
    "# 28. ECIGARET - E-ciggarette usage - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 29. _SMOKER3 - Smoking status - (Current smoker - now smokes every day, Current smoker - now smokes some days,\n",
    "# Former smoker, Never smoked, Don’t know/Refused/Missing\n",
    "# 30. _RFBING5 - Binge drinking status - (Yes, No, Don’t know/Refused/Missing)                \n",
    "\n",
    "# Columns to keep - Response variable\n",
    "# 31. CVDCRHD4 - Ever diagnosed with heart attack - (Yes, No, Don't know/Not sure, Refused)\n",
    "# 32. CVDCRHD4 - Ever diagnosed with angina/ coronary heart disease - (Yes, No, Don't know/Not sure, Refused)\n",
    "\n",
    "# For now we will predict heart disease\n",
    "df_clean_columns = df_clean[['SEXVAR', '_IMPAGE', '_IMPRACE', 'VETERAN3', 'WTKG3', '_IMPMRTL', '_RFBMI5', \n",
    "                             'HLTHPLN1', 'ADDEPEV3', 'DIABETE4', 'RMVTETH4', '_PHYS14D', '_MENT14D', '_TOTINDA',\n",
    "                             'PDIABTST', 'PREDIAB1', '_RFHLTH', 'BPHIGH4', 'CHECKUP1', 'LASTDEN4', 'FLUSHOT7', \n",
    "                             '_RFSEAT3', '_IMPEDUC', 'EMPLOY1', '_INCOMG', '_METSTAT', 'USENOW3', 'ECIGARET',\n",
    "                             '_SMOKER3', '_RFBING5', 'CVDCRHD4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all missing values\n",
    "df_cleaned = df_clean_columns.dropna(axis = 0).reset_index(drop = True)\n",
    "\n",
    "# Drop all rows that are Don't know/Not sure or Refused for column we are predicting\n",
    "df_cleaned = df_cleaned.loc[(df_cleaned['CVDCRHD4'] == 'No') | (df_cleaned['CVDCRHD4'] == 'Yes')]\n",
    "\n",
    "# Split into X and y\n",
    "X = df_cleaned.loc[:, df_cleaned.columns != 'CVDCRHD4']\n",
    "y = df_cleaned['CVDCRHD4']\n",
    "\n",
    "# Split the data into training and test data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imbalanced Data <br />\n",
    "Our data is very imbalanced. Only about 4% of the data is for people who have said that they have experienced a heart attack before. Because of this, we need to balance out our data. The two ways we will try to accomplish that is with SMOTE and upsampling. We will try both methods and see which method produces better scores when our models are built later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE using imblearn library\n",
    "\n",
    "os = SMOTENC(categorical_features = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29], random_state = 0)\n",
    "os_data_X , os_data_y = os.fit_resample(X_train, y_train)\n",
    "\n",
    "#Upsampling using resample\n",
    "#create two different dataframe of majority and minority class \n",
    "training_data = pd.DataFrame(X_train)\n",
    "training_data['CVDCRHD4'] = y_train\n",
    "df_majority = training_data[(training_data['CVDCRHD4']=='No')] \n",
    "df_minority = training_data[(training_data['CVDCRHD4']=='Yes')] \n",
    "# upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples= len(df_majority), # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority])\n",
    "X_train_upsampled = df_upsampled.loc[:, df_cleaned.columns != 'CVDCRHD4']\n",
    "y_train_upsampled = df_upsampled['CVDCRHD4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of components for FAMD\n",
    "optimal_components = pd.DataFrame(columns = ['num_components', 'explained_variance'])\n",
    "\n",
    "for i in range(1, 151):\n",
    "    \n",
    "    # Get val of x\n",
    "    num_components = i\n",
    "    \n",
    "    # Initialize FAMD\n",
    "    famd = FAMD(n_components = i, n_iter = 3, random_state = 42)\n",
    "    famd.fit_transform(X_train)\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    explained_variance = famd.explained_inertia_.sum()\n",
    "    \n",
    "    # Insert into dataframe\n",
    "    row = {'num_components': num_components, 'explained_variance' : explained_variance}\n",
    "    optimal_components = optimal_components.append(row, ignore_index = True)\n",
    "    \n",
    "fig = px.scatter(optimal_components, x = 'num_components', y = 'explained_variance')\n",
    "fig.show()\n",
    "\n",
    "# Print max \n",
    "optimal_components.loc[optimal_components['explained_variance'] == optimal_components['explained_variance'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAMD\n",
    "famd = FAMD(n_components = 120, n_iter = 3, random_state = 42)\n",
    "famd.fit_transform(X_train)\n",
    "\n",
    "famd_explained_variance = famd.explained_inertia_\n",
    "df_famd_explained = pd.DataFrame(famd_explained_variance)\n",
    "df_famd_explained['component'] = list(range(1, 121))\n",
    "df_famd_explained.columns = ['explained_variance', 'component']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance for each component\n",
    "\n",
    "fig = px.bar(df_famd_explained, x = 'component', y = 'explained_variance')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_correlations_comp_0 = famd.column_correlations(X_train)[0]\n",
    "famd_correlations_comp_0_abs = famd_correlations_comp_0.abs()\n",
    "famd_correlations_comp_0_top_20_labels = list(famd_correlations_comp_0_abs.sort_values(ascending=False)[0:10].index.values)\n",
    "famd_correlations_comp_0_top_10 = famd_correlations_comp_0.loc[famd_correlations_comp_0_top_20_labels]\n",
    "famd_correlations_top_0_labels_10 = list(famd_correlations_comp_0_top_10.index.values)\n",
    "famd_correlations_top_0_values_10 = list(famd_correlations_comp_0_top_10.values)\n",
    "famd_correlations_top_0_values_10\n",
    "\n",
    "famd_correlations_comp_1 = famd.column_correlations(X_train)[1]\n",
    "famd_correlations_comp_1_abs = famd_correlations_comp_1.abs()\n",
    "famd_correlations_comp_1_top_20_labels = list(famd_correlations_comp_1_abs.sort_values(ascending=False)[0:10].index.values)\n",
    "famd_correlations_comp_1_top_10 = famd_correlations_comp_1.loc[famd_correlations_comp_1_top_20_labels]\n",
    "famd_correlations_top_1_labels_10 = list(famd_correlations_comp_1_top_10.index.values)\n",
    "famd_correlations_top_1_values_10 = list(famd_correlations_comp_1_top_10.values)\n",
    "famd_correlations_comp_1_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_0 = ['COMPUTED WEIGHT IN KILOGRAMS', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - YES', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - NO', 'SEX OF RESPONDENT - MALE', 'SEX OF RESPONDENT - FEMALE', 'TOLD YOU HAVE HIGH BP - YES', 'TOLD YOU HAVE HIGH BP - NO', 'TOLD YOU HAVE PREDIABETES - YES', 'TOLD YOU HAVE PREDIABETES - NO', 'HAD TEST FOR HIGH BLOOD SUAGR IN LAST 3 YEARS - YES']\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=[famd_correlations_top_0_values_10],\n",
    "        x=labels_0,\n",
    "        y=['Component 0'],\n",
    "        colorscale='Viridis'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = ['COMPUTED WEIGHT IN KILOGRAMS', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - YES', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - NO', 'SEX OF RESPONDENT - MALE', 'SEX OF RESPONDENT - FEMALE', 'TOLD YOU HAVE HIGH BP - YES', 'TOLD YOU HAVE HIGH BP - NO', 'TOLD YOU HAVE PREDIABETES - YES', 'TOLD YOU HAVE PREDIABETES - NO', 'HAD TEST FOR HIGH BLOOD SUAGR IN LAST 3 YEARS - YES']\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=[famd_correlations_top_1_values_10],\n",
    "        x=labels_1,\n",
    "        y=['Component 1'],\n",
    "        colorscale='Viridis'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform SMOTE and upsampled training data\n",
    "X_train_os = famd.fit_transform(os_data_X)\n",
    "X_train_upsampled_transformed = famd.fit_transform(X_train_upsampled)\n",
    "X_train_imabalanced = famd.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data transformed using FAMD, let's see what kind of supervised learning scores we get when trying to classify people as at risk of heart attack or not at risk of heart attack. Using both the data balanced using SMOTE and upsampling to see which performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to get model scores\n",
    "def get_performance_scores(y_pred, y_true):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    return [f1, accuracy, precision, recall]\n",
    "\n",
    "def print_performance_scores(scores):\n",
    "    print(\"Accuracy Score = \" + str(scores[1]))\n",
    "    print(\"Precision Score = \" + str(scores[2]))\n",
    "    print(\"Recall Score = \" + str(scores[3]))\n",
    "    print(\"F1 Score = \" + str(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = famd.fit_transform(X_test)\n",
    "\n",
    "#Logistic Regression Using Original Dataset\n",
    "clf_lr = LogisticRegression(random_state = RANDOM_SEED, penalty='l2', class_weight = {'No': 0.75, 'Yes': 0.25},  C=0.25).fit(X_train_imabalanced, y_train)\n",
    "train_preds = (clf_lr.predict_proba(X_test_transformed)[:,1] >= 0.8).astype(int)\n",
    "train_preds = pd.DataFrame(train_preds, columns = ['val'])\n",
    "train_preds = train_preds['val'].replace(to_replace = [0, 1], value = ['No', 'Yes'])\n",
    "\n",
    "original_lr_scores = get_performance_scores(train_preds, y_test)\n",
    "\n",
    "#Logistic Regression Using SMOTE\n",
    "clf_lr = LogisticRegression(random_state = RANDOM_SEED, penalty='l2', class_weight = {'No': 0.75, 'Yes': 0.25},  C=0.25).fit(X_train_os, os_data_y.values.ravel())\n",
    "train_preds = (clf_lr.predict_proba(X_test_transformed)[:,1] >= 0.8).astype(int)\n",
    "train_preds = pd.DataFrame(train_preds, columns = ['val'])\n",
    "train_preds = train_preds['val'].replace(to_replace = [0, 1], value = ['No', 'Yes'])\n",
    "\n",
    "smote_lr_scores = get_performance_scores(train_preds, y_test)\n",
    "\n",
    "#Logistic Regression using upsampled data\n",
    "clf_lr = LogisticRegression(random_state = RANDOM_SEED, penalty='l2', class_weight = {'No': 0.75, 'Yes': 0.25},  C=0.25).fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "train_preds = (clf_lr.predict_proba(X_test_transformed)[:,1] >= 0.8).astype(int)\n",
    "train_preds = pd.DataFrame(train_preds, columns = ['val'])\n",
    "train_preds = train_preds['val'].replace(to_replace = [0, 1], value = ['No', 'Yes'])\n",
    "\n",
    "upsampled_lr_scores = get_performance_scores(train_preds, y_test)\n",
    "original_lr_scores.insert(0, 'Imbalanced')\n",
    "smote_lr_scores.insert(0, 'Balanced Using SMOTE')\n",
    "upsampled_lr_scores.insert(0, 'Upsampled using resample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Using Original Data\n",
    "random_forest = RandomForestClassifier(n_estimators= 200, min_samples_split= 2, min_samples_leaf = 1, max_depth = None, bootstrap = False, random_state = RANDOM_SEED )\n",
    "random_forest.fit(X_train_imabalanced, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test_transformed)\n",
    "    \n",
    "original_rf_scores = get_performance_scores(y_pred, y_test)\n",
    "\n",
    "#Random Forest Using SMOTE Data\n",
    "random_forest = RandomForestClassifier(n_estimators= 200, min_samples_split= 2, min_samples_leaf = 1, max_depth = None, bootstrap = False, random_state = RANDOM_SEED )\n",
    "random_forest.fit(X_train_os, os_data_y.values.ravel())\n",
    "\n",
    "y_pred = random_forest.predict(X_test_transformed)\n",
    "    \n",
    "smote_rf_scores = get_performance_scores(y_pred, y_test)\n",
    "\n",
    "#Random Forest Using Upsampled Data\n",
    "random_forest = RandomForestClassifier(n_estimators= 200, min_samples_split= 2, min_samples_leaf = 1, max_depth = None, bootstrap = False, random_state = RANDOM_SEED )\n",
    "random_forest.fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "\n",
    "y_pred = random_forest.predict(X_test_transformed)\n",
    "    \n",
    "upsampled_rf_scores = get_performance_scores(y_pred, y_test)\n",
    "original_rf_scores.insert(0, 'Imbalanced')\n",
    "smote_rf_scores.insert(0, 'Balanced Using SMOTE')\n",
    "upsampled_rf_scores.insert(0, 'Upsampled using resample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC Using Original dataset\n",
    "svc_model = SVC(C=100, gamma=0.1, kernel='poly', random_state=RANDOM_SEED).fit(X_train_imabalanced, y_train)\n",
    "y_pred = svc_model.predict(X_test_transformed)\n",
    "original_svc_scores = get_performance_scores(y_pred, y_test)\n",
    "\n",
    "#SVC Using SMOTE dataset\n",
    "svc_model = SVC(C=100, gamma=0.1, kernel='poly', random_state=RANDOM_SEED).fit(X_train_os, os_data_y.values.ravel())\n",
    "y_pred = svc_model.predict(X_test_transformed)\n",
    "smote_svc_scores = get_performance_scores(y_pred, y_test)\n",
    "\n",
    "#SVC Using Upsampled dataset\n",
    "svc_model = SVC(C=100, gamma=0.1, kernel='poly', random_state=RANDOM_SEED).fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "y_pred = svc_model.predict(X_test_transformed)\n",
    "upsampled_svc_scores = get_performance_scores(y_pred, y_test)\n",
    "\n",
    "original_svc_scores.insert(0, 'Imbalanced')\n",
    "smote_svc_scores.insert(0, 'Balanced Using SMOTE')\n",
    "upsampled_svc_scores.insert(0, 'Upsampled using resample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_columns = [\"Balancing Type\", \"F1 Score\", \"Accuracy Score\", \"Precision Score\", \"Recall Score\"]\n",
    "scores_rows = [\"Logistic Regression\", \"Logistic Regression\", \"Logistic Regression\", \"Random Forest\", \"Random Forest\", \"Random Forest\", \"SVC\", \"SVC\", \"SVC\"]\n",
    "scores = [original_lr_scores, smote_lr_scores, upsampled_lr_scores, original_rf_scores, smote_rf_scores, upsampled_rf_scores, original_svc_scores, smote_svc_scores, upsampled_svc_scores]\n",
    "\n",
    "scores_df = pd.DataFrame(data=scores, index=scores_rows, columns=scores_columns)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(scores_df, x=scores_df.index, y=\"F1 Score\",\n",
    "             color='Balancing Type', barmode='group', title=\"F1 Score By Model And Balance Type\", text_auto='.3', \n",
    "             labels={\"index\": \"Supervised Learning Model\", \"Balancing Type\": \"Balancing Method\"},\n",
    "             height=400)\n",
    "fig.update_layout(title_text='F1 Score By Model And Balancing Method', title_x=0.5)\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic regression model performed the best, let's see what the feature importance for that model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance\n",
    "from operator import itemgetter\n",
    "column_arr = X_train_upsampled_transformed.columns\n",
    "features_coeff = dict(zip(column_arr, clf_lr.coef_[0]))\n",
    "features_coeff_abs = {str(key) : abs(val) for key, val in features_coeff.items()}\n",
    "features_coeff_abs_top_10 = dict(sorted(features_coeff_abs.items(), key = itemgetter(1), reverse = True)[:10])\n",
    "print(features_coeff_abs_top_10)\n",
    "features_dict = {'Components': list(features_coeff_abs_top_10.keys()), 'coeffs': list(features_coeff_abs_top_10.values())}\n",
    "fig = px.bar(features_dict, x='Components', y='coeffs')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the performance is better when we upsample the data using the resample library than when we use SMOTE. We can try using upsampling with a different selection of initial columns to run FAMD to see if the results change. One way to select columns differently is by calcultating which columns have the highest correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode categorical variables as numeric to calculate correlations\n",
    "df_clean_categorical = df_clean.copy()\n",
    "cols = list(df_clean_categorical.columns)\n",
    "for col in cols:\n",
    "    if str(df_clean_categorical[col].dtype) == 'object':\n",
    "        df_clean_categorical[col] = df_clean_categorical[col].astype('category').cat.codes\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Create correlation matrix to find which features to use for mca\n",
    "df_clean_corr = df_clean_categorical.corrwith(df_clean_categorical[\"CVDCRHD4\"])\n",
    "df_clean_corr_abs = df_clean_corr.abs()\n",
    "df_clean_corr_abs.sort_values(inplace=True, ascending=False)\n",
    "df_clean_corr_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(df_clean_corr_abs[0:100].keys())\n",
    "feature_list.remove('CVDINFR4')\n",
    "feature_list.remove('_MICHD')\n",
    "feature_list\n",
    "\n",
    "df_clean_columns = df_clean[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Drop all missing values\n",
    "df_cleaned = df_clean_columns.dropna(axis = 0).reset_index(drop = True)\n",
    "\n",
    "# Drop all rows that are Don't know/Not sure or Refused for column we are predicting\n",
    "df_cleaned = df_cleaned.loc[(df_cleaned['CVDCRHD4'] == 'No') | (df_cleaned['CVDCRHD4'] == 'Yes')]\n",
    "\n",
    "# Split into X and y\n",
    "X = df_cleaned.loc[:, df_cleaned.columns != 'CVDCRHD4']\n",
    "y = df_cleaned['CVDCRHD4']\n",
    "\n",
    "# Split the data into training and test data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy= 'uniform').fit(famd.fit_transform(X_train),y_train)\n",
    "y_pred = dummy_clf.predict(famd.fit_transform(X_test))\n",
    "print_performance_scores(get_performance_scores(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd = FAMD(n_components = 120, n_iter = 3, random_state = 42)\n",
    "X_train_transformed = famd.fit_transform(X_train)\n",
    "X_test_transformed = famd.fit_transform(X_test)\n",
    "X_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(X_train)\n",
    "training_data['CVDCRHD4'] = y_train\n",
    "df_majority = training_data[(training_data['CVDCRHD4']=='No')] \n",
    "df_minority = training_data[(training_data['CVDCRHD4']=='Yes')] \n",
    "# upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples= len(df_majority), # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority])\n",
    "X_train_upsampled = df_upsampled.loc[:, df_cleaned.columns != 'CVDCRHD4']\n",
    "y_train_upsampled = df_upsampled['CVDCRHD4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled_transformed = famd.fit_transform(X_train_upsampled)\n",
    "clf_lr = LogisticRegression(random_state = RANDOM_SEED, penalty='l2', class_weight = {'No': 0.75, 'Yes': 0.25},  C=0.25).fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "train_preds = (clf_lr.predict_proba(X_test_transformed)[:,1] >= 0.8).astype(int)\n",
    "train_preds = pd.DataFrame(train_preds, columns = ['val'])\n",
    "train_preds = train_preds['val'].replace(to_replace = [0, 1], value = ['No', 'Yes'])\n",
    "\n",
    "upsampled_lr_scores_correlated = get_performance_scores(train_preds, y_test)\n",
    "print_performance_scores(upsampled_lr_scores_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance\n",
    "from operator import itemgetter\n",
    "column_arr = X_train_upsampled_transformed.columns\n",
    "features_coeff = dict(zip(column_arr, clf_lr.coef_[0]))\n",
    "features_coeff_abs = {str(key) : abs(val) for key, val in features_coeff.items()}\n",
    "features_coeff_abs_top_10 = dict(sorted(features_coeff_abs.items(), key = itemgetter(1), reverse = True)[:10])\n",
    "print(features_coeff_abs_top_10)\n",
    "features_dict = {'Components': list(features_coeff_abs_top_10.keys()), 'coeffs': list(features_coeff_abs_top_10.values())}\n",
    "fig = px.bar(features_dict, x='Components', y='coeffs', text_auto='.3')\n",
    "fig.update_layout(title_text='Feature Importance: Top 10 Feature Coefficients', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_correlations_comp_7 = famd.column_correlations(X_train)[7]\n",
    "famd_correlations_comp_7_abs = famd_correlations_comp_7.abs()\n",
    "famd_correlations_comp_7_top_20_labels = list(famd_correlations_comp_7_abs.sort_values(ascending=False)[0:10].index.values)\n",
    "famd_correlations_comp_7_top_10 = famd_correlations_comp_7.loc[famd_correlations_comp_7_top_20_labels]\n",
    "famd_correlations_top_10_labels_7 = list(famd_correlations_comp_7_top_10.index.values)\n",
    "famd_correlations_top_10_values_7 = list(famd_correlations_comp_7_top_10.values)\n",
    "\n",
    "\n",
    "\n",
    "famd_correlations_comp_5 = famd.column_correlations(X_train)[5]\n",
    "famd_correlations_comp_5_abs = famd_correlations_comp_5.abs()\n",
    "famd_correlations_comp_5_top_20_labels = list(famd_correlations_comp_5_abs.sort_values(ascending=False)[0:10].index.values)\n",
    "famd_correlations_comp_5_top_10 = famd_correlations_comp_5.loc[famd_correlations_comp_5_top_20_labels]\n",
    "famd_correlations_top_10_labels_5 = list(famd_correlations_comp_5_top_10.index.values)\n",
    "famd_correlations_top_10_values_5 = list(famd_correlations_comp_5_top_10.values)\n",
    "\n",
    "\n",
    "famd_correlations_comp_10 = famd.column_correlations(X_train)[10]\n",
    "famd_correlations_comp_10_abs = famd_correlations_comp_10.abs()\n",
    "famd_correlations_comp_10_top_20_labels = list(famd_correlations_comp_10_abs.sort_values(ascending=False)[0:10].index.values)\n",
    "famd_correlations_comp_10_top_10 = famd_correlations_comp_10.loc[famd_correlations_comp_10_top_20_labels]\n",
    "famd_correlations_top_10_labels_10 = list(famd_correlations_comp_10_top_10.index.values)\n",
    "famd_correlations_top_10_values_10 = list(famd_correlations_comp_10_top_10.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_7 = ['ANNUAL SEQUENCE NUMBER', 'PRIMARY SAMPLING UNIT', 'QUESTIONNAIRE VERSION IDENTIFIER', 'FINAL WEIGHT: LAND-LINE AND CELL-PHONE DATA', 'Final Adjusted Weight for Content on Two of Three Splits','COMPUTED WEIGHT IN KILOGRAMS', 'OVERWEIGHT OR OBESE - OBESE', 'AGE TOLD HAD CANCER - >= 97', 'AGE TOLD HAD CANCER - MISSING', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - No']\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=[famd_correlations_top_10_values_7],\n",
    "        x=labels_7,\n",
    "        y=['Component 7'],\n",
    "        colorscale='Viridis'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_5 = ['COMPUTED WEIGHT IN KILOGRAMS', 'COMPUTED BODY MASS INDEX CATEGORIES - OBESE', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - Yes', 'OVERWEIGHT OR OBESE CALCULATED VARIABLE - No', 'COMPUTED BODY MASS INDEX CATEGORIES - NORMAL WEIGHT', 'ANNUAL SEQUENCE NUMBER', 'PRIMARY SAMPLING UNIT', 'QUESTIONNAIRE VERSION IDENTIFIER', 'TOLD YOU HAVE DIABETES - No', 'NUMBER OF DAYS PHYSICAL HEALTH NOT GOOD - None']\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=[famd_correlations_top_10_values_5],\n",
    "        x=labels_5,\n",
    "        y=['Component 5'],\n",
    "        colorscale='Viridis'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state = RANDOM_SEED, n_estimators= 200, min_samples_split= 2, min_samples_leaf = 1, max_depth = None, bootstrap = False, class_weight={'No': 0.25, 'Yes': 0.75})\n",
    "random_forest.fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "\n",
    "y_pred = random_forest.predict(X_test_transformed)\n",
    "    \n",
    "upsampled_rf_scores_correlated = get_performance_scores(y_pred, y_test)\n",
    "print_performance_scores(upsampled_rf_scores_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 3 cells contain our grid searches for hyperparameter tuning. This code is commented out since it takes a long time to run and we have already tuned the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_SEED).fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "\n",
    "grid_values = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    " }\n",
    "\n",
    "rand_search_clf = RandomizedSearchCV(estimator = rf_clf, param_distributions = grid_values, n_iter = 10, cv = 3, verbose=2, scoring='f1_macro', random_state=RANDOM_SEED, n_jobs = -1)\n",
    "\n",
    "my_list = list(range(100))\n",
    "for x in tqdm(my_list):\n",
    "    rand_search_clf.fit(X_train_upsampled_transformed[0:10000], y_train_upsampled[0:10000])\n",
    "print(rand_search_clf.best_estimator_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "grid = RandomizedSearchCV(SVC(random_state=RANDOM_SEED),param_grid,refit=True,verbose=2, scoring='f1_macro')\n",
    "\n",
    "my_list = list(range(100))\n",
    "for x in tqdm(my_list):\n",
    "    grid.fit(X_train_transformed[0:10000],y_train_upsampled[0:10000])\n",
    "\n",
    "print(grid.best_estimator_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Do grid search for hyperparameter tuning\n",
    "clf = LogisticRegression(random_state = RANDOM_SEED, solver='liblinear')\n",
    "grid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'f1_macro')\n",
    "my_list = list(range(100))\n",
    "for x in tqdm(my_list):\n",
    "    grid_clf_acc.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_acc = grid_clf_acc.predict(X_test_transformed)\n",
    "\n",
    "print_performance_scores(get_performance_scores(y_pred_acc, y_test))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=100, gamma=0.1, kernel='poly', random_state=RANDOM_SEED).fit(X_train_upsampled_transformed, y_train_upsampled)\n",
    "y_pred = svc_model.predict(X_test_transformed)\n",
    "upsampled_svc_scores_correlated = get_performance_scores(y_pred, y_test)\n",
    "print_performance_scores(upsampled_svc_scores_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_lr_scores.remove(\"Upsampled using resample\")\n",
    "upsampled_rf_scores.remove(\"Upsampled using resample\")\n",
    "upsampled_svc_scores.remove(\"Upsampled using resample\")\n",
    "upsampled_lr_scores.insert(0, \"Manually Selected Columns\")\n",
    "upsampled_rf_scores.insert(0, \"Manually Selected Columns\")\n",
    "upsampled_svc_scores.insert(0, \"Manually Selected Columns\")\n",
    "upsampled_lr_scores_correlated.insert(0, \"Most Correlated Columns\")\n",
    "upsampled_rf_scores_correlated.insert(0, \"Most Correlated Columns\")\n",
    "upsampled_svc_scores_correlated.insert(0, \"Most Correlated Columns\")\n",
    "scores_columns = [\"Input Columns\", \"F1 Score\", \"Accuracy Score\", \"Precision Score\", \"Recall Score\"]\n",
    "scores_rows = [\"Logistic Regression\", \"Logistic Regression\", \"Random Forest\", \"Random Forest\", \"SVC\", \"SVC\"]\n",
    "scores = [upsampled_lr_scores, upsampled_lr_scores_correlated, upsampled_rf_scores, upsampled_rf_scores_correlated, upsampled_svc_scores, upsampled_svc_scores_correlated]\n",
    "print(scores)\n",
    "scores_df = pd.DataFrame(data=scores, index=scores_rows, columns=scores_columns)\n",
    "scores_df\n",
    "\n",
    "fig = px.bar(scores_df, x=scores_df.index, y=\"F1 Score\",\n",
    "             color='Input Columns', barmode='group', title=\"F1 Score By Model And Balance Type\", text_auto='.3', \n",
    "             labels={\"index\": \"Supervised Learning Model\", \"Input Columns\": \"Initial Column Selection\"},\n",
    "             height=400)\n",
    "fig.update_layout(title_text='F1 Score By Model And Column Selection Method', title_x=0.5)\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eabca979b0553fa6d87e9a00c352604d3b703d4afc9641643dd42376492b80f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
